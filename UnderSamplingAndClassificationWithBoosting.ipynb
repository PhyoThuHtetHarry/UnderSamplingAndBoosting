{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as sklm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = np.array(pd.read_csv('Credit_Features.csv'))\n",
    "Labels  = np.array(pd.read_csv('Credit_Labels.csv'))\n",
    "\n",
    "Labels = Labels.reshape(Labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as nr\n",
    "Features_0 =  Features[Labels == 0,:] \n",
    "Features_1 =  Features[Labels == 1,:]\n",
    "Labels_0   =  Labels[Labels == 0]\n",
    "Labels_1   =  Labels[Labels == 1]\n",
    "indx       =  nr.choice(Features_0.shape[0],Features_1.shape[0],replace = True)\n",
    "temp_Labels   = np.concatenate((Labels_1,Labels_0[indx]),axis = 0)\n",
    "temp_Features = np.concatenate((Features_1,Features_0[indx,:]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.7083687   1.16304581 -0.87018333 ...,  0.          1.          0.        ]\n",
      " [ 0.51728944  0.90474274  0.02414692 ...,  0.          0.          1.        ]\n",
      " [ 0.90073132  0.99762148  0.91847717 ...,  1.          0.          0.        ]\n",
      " ..., \n",
      " [-0.67378981 -0.1815588  -0.87018333 ...,  0.          1.          0.        ]\n",
      " [ 0.28783389  0.03312571  0.91847717 ...,  0.          1.          0.        ]\n",
      " [-0.29034793  0.14853305 -0.87018333 ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(temp_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "nr.seed(123)\n",
    "inside = ms.KFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "nr.seed(99)\n",
    "outside = ms.KFold(n_splits = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'learning_rate':[1,10,0.1]}\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "nr.seed(0)\n",
    "estimator = AdaBoostClassifier()\n",
    "estimator =  ms.GridSearchCV(estimator = estimator, cv = inside, param_grid = param_grid, scoring = 'roc_auc', \n",
    "                                return_train_score = True)\n",
    "\n",
    "estimator.fit(temp_Features,temp_Labels)\n",
    "print(estimator.best_estim,ator_.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr.seed(9999)\n",
    "estimate = ms.cross_val_score(estimator, temp_Features, temp_Labels, cv = outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Performance Metirc = 0.769\n",
      "Std of Performance Metric = 0.043\n"
     ]
    }
   ],
   "source": [
    "print (\"Mean of Performance Metirc = %4.3f\"%np.mean(estimate))\n",
    "print(\"Std of Performance Metric = %4.3f\"%np.std(estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1   0.821\n",
      "Fold  2   0.792\n",
      "Fold  3   0.781\n",
      "Fold  4   0.687\n",
      "Fold  5   0.765\n",
      "Fold  6   0.752\n",
      "Fold  7   0.737\n",
      "Fold  8   0.778\n",
      "Fold  9   0.730\n",
      "Fold 10   0.845\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(estimate):\n",
    "    print(\"Fold %2d   %4.3f\"%(i+1,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "nr.seed(777)\n",
    "indx = range(temp_Features.shape[0])\n",
    "print(temp_Features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(777)\n",
    "indx = ms.train_test_split(indx, test_size = 300 )\n",
    "train_X = temp_Features[indx[0],:]\n",
    "test_X  = temp_Features[indx[1],:]\n",
    "train_y = np.ravel(temp_Labels[indx[0]])\n",
    "test_y  = np.ravel(temp_Labels[indx[1]])\n",
    "\n",
    "model = AdaBoostClassifier(learning_rate = estimator.best_estimator_.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.1, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive        92                58\n",
      "Actual negative        32               118\n",
      "\n",
      "Accuracy        0.70\n",
      "AUC             0.75\n",
      "Macro precision 0.71\n",
      "Macro recall    0.70\n",
      " \n",
      "           Positive      Negative\n",
      "Num case      150           150\n",
      "Precision    0.74          0.67\n",
      "Recall       0.61          0.79\n",
      "F1           0.67          0.72\n"
     ]
    }
   ],
   "source": [
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "probabilities = model.predict_proba(test_X)\n",
    "print_metrics(test_y, probabilities, 0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
